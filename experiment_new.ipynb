{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available and will be used.\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"GPU is available and will be used.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available and will be used.\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    BertTokenizer, BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling,\n",
    "    TapasTokenizer, TapasForMaskedLM,\n",
    "    AdamW, get_scheduler\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from model_complete import JSONBERT_COMPLETE\n",
    "from dataset import JSONDataset, JSONDataCollator, create_data\n",
    "\n",
    "import sys\n",
    "sys.path.append('/root/woojun/')\n",
    "\n",
    "from utils import (\n",
    "    _serialize_vanilla,\n",
    "    _serialize,\n",
    "    tokenize_table,\n",
    "    get_table_embedding,\n",
    "    evaluate_masked_prediction,\n",
    "    prepare_Xy,\n",
    "    train_eval_rf\n",
    ")\n",
    "\n",
    "from utils import (\n",
    "    _serialize_vanilla,\n",
    "    _serialize,\n",
    "    tokenize_table,\n",
    "    get_table_embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer & config\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "config = BertConfig.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TapasForMaskedLM(\n",
       "  (tapas): TapasModel(\n",
       "    (embeddings): TapasEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(1024, 768)\n",
       "      (token_type_embeddings_0): Embedding(3, 768)\n",
       "      (token_type_embeddings_1): Embedding(256, 768)\n",
       "      (token_type_embeddings_2): Embedding(256, 768)\n",
       "      (token_type_embeddings_3): Embedding(2, 768)\n",
       "      (token_type_embeddings_4): Embedding(256, 768)\n",
       "      (token_type_embeddings_5): Embedding(256, 768)\n",
       "      (token_type_embeddings_6): Embedding(10, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): TapasEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x TapasLayer(\n",
       "          (attention): TapasAttention(\n",
       "            (self): TapasSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): TapasSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): TapasIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): TapasOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): TapasOnlyMLMHead(\n",
       "    (predictions): TapasLMPredictionHead(\n",
       "      (transform): TapasPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Models\n",
    "\n",
    "# BERT\n",
    "bert_base = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "bert_base = bert_base.to(device)\n",
    "\n",
    "# TaPas\n",
    "tapas_name = \"google/tapas-base-masklm\"\n",
    "tapas_tokenizer = TapasTokenizer.from_pretrained(tapas_name)\n",
    "tapas = TapasForMaskedLM.from_pretrained(tapas_name)\n",
    "tapas.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key embeddings are trainable!\n",
      "Pre-trained JSONBERT loaded from ./models/movie_complete/epoch-9\n",
      "Key embeddings are trainable!\n",
      "Pre-trained JSONBERT_INTERPOLATE loaded from ./models/movie_no_cl/epoch-9\n",
      "Key embeddings are trainable!\n",
      "Pre-trained JSONBERT_NEWLOSS loaded from ./models/movie_alpha_0/epoch-9\n",
      "Key embeddings are trainable!\n",
      "Pre-trained JSONBERT_NEWLOSS loaded from ./models/movie_alpha_1/epoch-9\n"
     ]
    }
   ],
   "source": [
    "from no_cl import JSONBERT_INTERPOLATE\n",
    "from no_ip_alpha_0 import JSONBERT_NEWLOSS_0\n",
    "from no_ip_alpha_1 import JSONBERT_NEWLOSS_1\n",
    "\n",
    "ours_path_movie = './models/movie_complete/epoch-9'\n",
    "no_cl_path_movie = './models/movie_no_cl/epoch-9'\n",
    "alpha_0_path_movie = './models/movie_alpha_0/epoch-9'\n",
    "alpha_1_path_movie = './models/movie_alpha_1/epoch-9'\n",
    "no_hel_path_movie = './models/movie_no_hel/epoch-9'\n",
    "bert_path_movie = './models/movie_bert/epoch-9'\n",
    "\n",
    "ours_movie = JSONBERT_COMPLETE(config, tokenizer, ours_path_movie)\n",
    "ours_movie = ours_movie.to(device)\n",
    "\n",
    "no_cl_movie = JSONBERT_INTERPOLATE(config, tokenizer, no_cl_path_movie)\n",
    "no_cl_movie = no_cl_movie.to(device)\n",
    "\n",
    "alpha_0_movie = JSONBERT_NEWLOSS_0(config, tokenizer, alpha_0_path_movie)\n",
    "alpha_0_movie = alpha_0_movie.to(device)\n",
    "\n",
    "alpha_1_movie = JSONBERT_NEWLOSS_1(config, tokenizer, alpha_1_path_movie)\n",
    "alpha_1_movie = alpha_1_movie.to(device)\n",
    "\n",
    "no_hel_movie = BertForMaskedLM.from_pretrained(no_hel_path_movie, local_files_only=True)\n",
    "no_hel_movie = no_hel_movie.to(device)\n",
    "\n",
    "bert_movie = BertForMaskedLM.from_pretrained(bert_path_movie, local_files_only=True)\n",
    "bert_movie = bert_movie.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key embeddings are trainable!\n",
      "Pre-trained JSONBERT loaded from ./models/product_complete/epoch-9\n",
      "Key embeddings are trainable!\n",
      "Pre-trained JSONBERT_INTERPOLATE loaded from ./models/product_no_cl/epoch-9\n",
      "Key embeddings are trainable!\n",
      "Pre-trained JSONBERT_NEWLOSS loaded from ./models/product_alpha_0/epoch-9\n",
      "Key embeddings are trainable!\n",
      "Pre-trained JSONBERT_NEWLOSS loaded from ./models/product_alpha_1/epoch-9\n"
     ]
    }
   ],
   "source": [
    "# Product\n",
    "\n",
    "ours_path_product = './models/product_complete/epoch-9'\n",
    "no_cl_path_product = './models/product_no_cl/epoch-9'\n",
    "alpha_0_path_product = './models/product_alpha_0/epoch-9'\n",
    "alpha_1_path_product = './models/product_alpha_1/epoch-9'\n",
    "no_hel_path_product = './models/product_no_hel/epoch-9'\n",
    "bert_path_product = './models/product_bert/epoch-9'\n",
    "\n",
    "ours_product = JSONBERT_COMPLETE(config, tokenizer, ours_path_product)\n",
    "ours_product = ours_product.to(device)\n",
    "\n",
    "no_cl_product = JSONBERT_INTERPOLATE(config, tokenizer, no_cl_path_product)\n",
    "no_cl_product = no_cl_product.to(device)\n",
    "\n",
    "alpha_0_product = JSONBERT_NEWLOSS_0(config, tokenizer, alpha_0_path_product)\n",
    "alpha_0_product = alpha_0_product.to(device)\n",
    "\n",
    "alpha_1_product = JSONBERT_NEWLOSS_1(config, tokenizer, alpha_1_path_product)\n",
    "alpha_1_product = alpha_1_product.to(device)\n",
    "\n",
    "no_hel_product = BertForMaskedLM.from_pretrained(no_hel_path_product, local_files_only=True)\n",
    "no_hel_product = no_hel_product.to(device)\n",
    "\n",
    "bert_product = BertForMaskedLM.from_pretrained(bert_path_product, local_files_only=True)\n",
    "bert_product = bert_product.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping non-English table: Movie_adorocinema.com_October2023.json\n",
      "Skipping non-English table: Movie_afisha.ru_October2023.json\n",
      "Skipping non-English table: Movie_ak.sv_October2023.json\n",
      "Skipping non-English table: Movie_allcinema.net_October2023.json\n",
      "Skipping non-English table: Movie_allocine.fr_October2023.json\n",
      "Skipping non-English table: Movie_arte.tv_October2023.json\n",
      "Skipping non-English table: Movie_cinecitta.de_October2023.json\n",
      "Skipping non-English table: Movie_cinefil.com_October2023.json\n",
      "Skipping non-English table: Movie_cinema-rank.net_October2023.json\n",
      "Skipping non-English table: Movie_cinematoday.jp_October2023.json\n",
      "Skipping non-English table: Movie_comingsoon.it_October2023.json\n",
      "Skipping non-English table: Movie_cpop.it_October2023.json\n",
      "Skipping non-English table: Movie_crank-in.net_October2023.json\n",
      "Skipping non-English table: Movie_dok-film.net_October2023.json\n",
      "Skipping non-English table: Movie_domkino.tv_October2023.json\n",
      "Skipping non-English table: Movie_ecranlarge.com_October2023.json\n",
      "Skipping non-English table: Movie_esensja.pl_October2023.json\n",
      "Skipping non-English table: Movie_expansion.com_October2023.json\n",
      "Skipping non-English table: Movie_filmaffinity.com_October2023.json\n",
      "Skipping non-English table: Movie_filmstarts.de_October2023.json\n",
      "Skipping non-English table: Movie_hinet.net_October2023.json\n",
      "Skipping non-English table: Movie_ign.com_October2023.json\n",
      "Skipping non-English table: Movie_jfc.org.il_October2023.json\n",
      "Skipping non-English table: Movie_kino-zeit.de_October2023.json\n",
      "Skipping non-English table: Movie_kino.de_October2023.json\n",
      "Skipping non-English table: Movie_kinoheld.de_October2023.json\n",
      "Skipping non-English table: Movie_kpn.com_October2023.json\n",
      "Skipping non-English table: Movie_lacinetek.com_October2023.json\n",
      "Skipping non-English table: Movie_mail.ru_October2023.json\n",
      "Skipping non-English table: Movie_moviebreak.de_October2023.json\n",
      "Skipping non-English table: Movie_moviejones.de_October2023.json\n",
      "Skipping non-English table: Movie_moviemeter.nl_October2023.json\n",
      "Skipping non-English table: Movie_moviewalker.jp_October2023.json\n",
      "Skipping non-English table: Movie_movistarplus.es_October2023.json\n",
      "Skipping non-English table: Movie_mymovies.it_October2023.json\n",
      "Skipping non-English table: Movie_myvideo.net.tw_October2023.json\n",
      "Skipping non-English table: Movie_nfb.ca_October2023.json\n",
      "Skipping non-English table: Movie_nientepopcorn.it_October2023.json\n",
      "Skipping non-English table: Movie_onf.ca_October2023.json\n",
      "Skipping non-English table: Movie_ouest-france.fr_October2023.json\n",
      "Skipping non-English table: Movie_ovideo.ru_October2023.json\n",
      "Skipping non-English table: Movie_pb.wtf_October2023.json\n",
      "Skipping non-English table: Movie_pix-geeks.com_October2023.json\n",
      "Skipping non-English table: Movie_premiere.fr_October2023.json\n",
      "Skipping non-English table: Movie_scary-movies.de_October2023.json\n",
      "Skipping non-English table: Movie_screenweek.it_October2023.json\n",
      "Skipping non-English table: Movie_spielfilm.de_October2023.json\n",
      "Skipping non-English table: Movie_telerama.fr_October2023.json\n",
      "Skipping non-English table: Movie_timeout.ru_October2023.json\n",
      "Skipping non-English table: Movie_watcha.com_October2023.json\n",
      "Skipping non-English table: Product_10x10.co.kr_October2023.json\n",
      "Skipping non-English table: Product_all.biz_October2023.json\n",
      "Skipping non-English table: Product_allegro.pl_October2023.json\n",
      "Skipping non-English table: Product_avito.ru_October2023.json\n",
      "Skipping non-English table: Product_com.ru_October2023.json\n",
      "Skipping non-English table: Product_docomo.ne.jp_October2023.json\n",
      "Skipping non-English table: Product_elektronikai-hulladek-felvasarlas.hu_October2023.json\n",
      "Skipping non-English table: Product_eltiempo.com_October2023.json\n",
      "Skipping non-English table: Product_fateful.hu_October2023.json\n",
      "Skipping non-English table: Product_havidijas-keresooptimalizalas.hu_October2023.json\n",
      "Skipping non-English table: Product_iherb.com_October2023.json\n",
      "Skipping non-English table: Product_line.me_October2023.json\n",
      "Skipping non-English table: Product_numizmatik.ru_October2023.json\n",
      "Skipping non-English table: Product_odoo.com_October2023.json\n",
      "Skipping non-English table: Product_pinkoi.com_October2023.json\n",
      "Skipping non-English table: Product_pp.ua_October2023.json\n",
      "Skipping non-English table: Product_profi.ru_October2023.json\n",
      "Skipping non-English table: Product_robotshop.com_October2023.json\n",
      "Skipping non-English table: Product_semana.com_October2023.json\n",
      "Skipping non-English table: Product_spb.ru_October2023.json\n",
      "Skipping non-English table: Product_t-online.de_October2023.json\n",
      "Skipping non-English table: Product_taobao.com_October2023.json\n",
      "Skipping non-English table: Product_tiki.vn_October2023.json\n",
      "Skipping non-English table: Product_todocoleccion.net_October2023.json\n",
      "Skipping non-English table: Product_udg.mx_October2023.json\n",
      "Skipping non-English table: Product_weboldal-webaruhaz-keszites-budapest.com_October2023.json\n",
      "Skipping non-English table: Product_yahoo.com_October2023.json\n",
      "Skipping non-English table: Product_yamaha.com_October2023.json\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "\n",
    "pretraining_movie_path = './data/pretraining_data_movie.jsonl'\n",
    "movie_path = './data/Movie_top100'\n",
    "movie = create_data(movie_path, path_is=\"test\", sample_num=20, pretraining_path=pretraining_movie_path)\n",
    "\n",
    "pretraining_product_path = './data/pretraining_data_product.jsonl'\n",
    "product_path = './data/Product_top100'\n",
    "product = create_data(product_path, path_is=\"test\", sample_num=20, pretraining_path=pretraining_product_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_clustering_experiment(X, y):\n",
    "#     # Convert labels to binary format\n",
    "#     unique_labels = np.unique(y)\n",
    "#     n_clusters = len(unique_labels)\n",
    "\n",
    "#     # Run K-Means clustering with the number of ground truth clusters\n",
    "#     kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "#     kmeans.fit(X)\n",
    "\n",
    "#     # Predict clusters for test data\n",
    "#     cluster_labels = kmeans.predict(X)\n",
    "\n",
    "#     # Evaluate clustering performance\n",
    "#     nmi = normalized_mutual_info_score(y, cluster_labels)\n",
    "#     print(f\"Normalized Mutual Information (NMI) on test data: {nmi:.2f}\")\n",
    "\n",
    "#     ari = adjusted_rand_score(y, cluster_labels)\n",
    "#     print(f\"Adjusted Rand Index (ARI) on test data: {ari:.2f}\")\n",
    "\n",
    "#     # Verify data sizes\n",
    "#     print(f\"X shape: {X.shape}, y length: {len(y)}\")\n",
    "\n",
    "#     # Check unique labels\n",
    "#     print(f\"Unique labels in y: {np.unique(y)}\")\n",
    "\n",
    "\n",
    "#     # Visualize clustering\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     plt.scatter(X[:, 0], X[:, 1], c=cluster_labels, cmap='tab20', marker='o', edgecolor='k')\n",
    "#     plt.title(f\"K-Means Clustering Results with {n_clusters} Clusters\")\n",
    "#     plt.xlabel(\"Feature 1\")\n",
    "#     plt.ylabel(\"Feature 2\")\n",
    "#     plt.legend(['Cluster ' + str(i) for i in range(n_clusters)])\n",
    "#     plt.show()\n",
    "\n",
    "# functions\n",
    "\n",
    "# def prepare_Xy(df, model, tokenizer, target='filename', seed=42):\n",
    "#     data = df.to_dict(orient=\"records\")\n",
    "#     y = df[target].values\n",
    "#     X = np.array([get_table_embedding(entry, model, tokenizer, target) for entry in data])\n",
    "\n",
    "#     return X, y\n",
    "\n",
    "# movie_df = pd.DataFrame(movie)\n",
    "# sampled_filenames = ['Movie_telescopefilm.com_October2023.json', 'Movie_tubitv.com_October2023.json']\n",
    "# sampled_data = [row for row in movie if row[\"filename\"] in sampled_filenames]\n",
    "# sampled_df = pd.DataFrame(sampled_data)\n",
    "\n",
    "# X, y = prepare_Xy(sampled_df, notsure_movie, tokenizer, 'filename')\n",
    "# run_clustering_experiment(X, y)\n",
    "\n",
    "# X, y = prepare_Xy(sampled_df, ours_movie, tokenizer, 'filename')\n",
    "# run_clustering_experiment(X, y)\n",
    "\n",
    "# X, y = prepare_Xy(sampled_df, bert_base, tokenizer, 'filename')\n",
    "# run_clustering_experiment(X, y)\n",
    "\n",
    "# X, y = prepare_Xy(sampled_df, tapas, tapas_tokenizer, 'filename')\n",
    "# run_clustering_experiment(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export embeddings\n",
    "\n",
    "# data = sampled_df.to_dict(orient=\"records\")\n",
    "# sampled_df['our_embeddings'] = [get_table_embedding(entry, ours_movie, tokenizer, 'filename') for entry in data]\n",
    "# sampled_df['bert_embeddings'] = [get_table_embedding(entry, bert_base, tokenizer, 'filename') for entry in data]\n",
    "\n",
    "# csv_export_path = 'embeddings_from_our_model.csv'\n",
    "# sampled_df.to_csv(csv_export_path, index=False)\n",
    "# print(f'DataFrame exported to {csv_export_path}')\n",
    "\n",
    "# json_export_path = 'embeddings_from_our_model.json'\n",
    "# sampled_df.to_json(json_export_path, orient='records', indent=4)\n",
    "# print(f'DataFrame exported to {json_export_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct / Total: 1843/3183\n",
      "Model Accuracy on Masked Key Prediction: 0.5790%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/tapas/tokenization_tapas.py:2699: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text = normalize_for_match(row[col_index].text)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/tapas/tokenization_tapas.py:1493: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  cell = row[col_index]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct / Total: 1093/3869\n",
      "Model Accuracy on Masked Key Prediction: 0.2825%\n",
      "Correct / Total: 1660/2849\n",
      "Model Accuracy on Masked Value Prediction: 0.5827%\n",
      "Correct / Total: 13617/32659\n",
      "Model Accuracy on Masked Value Prediction: 0.4169%\n",
      "Correct / Total: 3175/3183\n",
      "Model Accuracy on Masked Key Prediction: 0.9975%\n",
      "Correct / Total: 3171/3183\n",
      "Model Accuracy on Masked Key Prediction: 0.9962%\n",
      "Correct / Total: 2562/3183\n",
      "Model Accuracy on Masked Key Prediction: 0.8049%\n",
      "Correct / Total: 3177/3183\n",
      "Model Accuracy on Masked Key Prediction: 0.9981%\n",
      "Correct / Total: 3177/3183\n",
      "Model Accuracy on Masked Key Prediction: 0.9981%\n",
      "Correct / Total: 2552/3183\n",
      "Model Accuracy on Masked Key Prediction: 0.8018%\n",
      "Correct / Total: 2204/2849\n",
      "Model Accuracy on Masked Value Prediction: 0.7736%\n",
      "Correct / Total: 2176/2849\n",
      "Model Accuracy on Masked Value Prediction: 0.7638%\n",
      "Correct / Total: 2160/2849\n",
      "Model Accuracy on Masked Value Prediction: 0.7582%\n",
      "Correct / Total: 2172/2849\n",
      "Model Accuracy on Masked Value Prediction: 0.7624%\n",
      "Correct / Total: 2220/2849\n",
      "Model Accuracy on Masked Value Prediction: 0.7792%\n",
      "Correct / Total: 1774/2849\n",
      "Model Accuracy on Masked Value Prediction: 0.6227%\n"
     ]
    }
   ],
   "source": [
    "# In-domain: Movie\n",
    "\n",
    "# Pre-trained: BERT, TaPas, TaBERT\n",
    "evaluate_masked_prediction(movie, 'Key', bert_base, tokenizer)\n",
    "evaluate_masked_prediction(movie, 'Key', tapas, tapas_tokenizer)\n",
    "\n",
    "evaluate_masked_prediction(movie, 'Value', bert_base, tokenizer)\n",
    "evaluate_masked_prediction(movie, 'Value', tapas, tapas_tokenizer)\n",
    "\n",
    "# Domain-specific pre-trained: Ours, No CL, No IP_a0, No IP_a1, No HEL, trained BERT\n",
    "evaluate_masked_prediction(movie, 'Key', ours_movie, tokenizer)\n",
    "evaluate_masked_prediction(movie, 'Key', no_cl_movie, tokenizer)\n",
    "evaluate_masked_prediction(movie, 'Key', alpha_0_movie, tokenizer)\n",
    "evaluate_masked_prediction(movie, 'Key', alpha_1_movie, tokenizer)\n",
    "evaluate_masked_prediction(movie, 'Key', no_hel_movie, tokenizer)\n",
    "evaluate_masked_prediction(movie, 'Key', bert_movie, tokenizer)\n",
    "\n",
    "evaluate_masked_prediction(movie, 'Value', ours_movie, tokenizer)\n",
    "evaluate_masked_prediction(movie, 'Value', no_cl_movie, tokenizer)\n",
    "evaluate_masked_prediction(movie, 'Value', alpha_0_movie, tokenizer)\n",
    "evaluate_masked_prediction(movie, 'Value', alpha_1_movie, tokenizer)\n",
    "evaluate_masked_prediction(movie, 'Value', no_hel_movie, tokenizer)\n",
    "evaluate_masked_prediction(movie, 'Value', bert_movie, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct / Total: 1920/4140\n",
      "Model Accuracy on Masked Key Prediction: 0.4638%\n",
      "Correct / Total: 1053/4448\n",
      "Model Accuracy on Masked Key Prediction: 0.2367%\n",
      "Correct / Total: 1430/2490\n",
      "Model Accuracy on Masked Value Prediction: 0.5743%\n",
      "Correct / Total: 18640/48358\n",
      "Model Accuracy on Masked Value Prediction: 0.3855%\n",
      "Correct / Total: 4088/4140\n",
      "Model Accuracy on Masked Key Prediction: 0.9874%\n",
      "Correct / Total: 4090/4140\n",
      "Model Accuracy on Masked Key Prediction: 0.9879%\n",
      "Correct / Total: 3062/4140\n",
      "Model Accuracy on Masked Key Prediction: 0.7396%\n",
      "Correct / Total: 4096/4140\n",
      "Model Accuracy on Masked Key Prediction: 0.9894%\n",
      "Correct / Total: 4104/4140\n",
      "Model Accuracy on Masked Key Prediction: 0.9913%\n",
      "Correct / Total: 3171/4140\n",
      "Model Accuracy on Masked Key Prediction: 0.7659%\n",
      "Correct / Total: 1775/2490\n",
      "Model Accuracy on Masked Value Prediction: 0.7129%\n",
      "Correct / Total: 1753/2490\n",
      "Model Accuracy on Masked Value Prediction: 0.7040%\n",
      "Correct / Total: 1771/2490\n",
      "Model Accuracy on Masked Value Prediction: 0.7112%\n",
      "Correct / Total: 1753/2490\n",
      "Model Accuracy on Masked Value Prediction: 0.7040%\n",
      "Correct / Total: 1774/2490\n",
      "Model Accuracy on Masked Value Prediction: 0.7124%\n",
      "Correct / Total: 1523/2490\n",
      "Model Accuracy on Masked Value Prediction: 0.6116%\n"
     ]
    }
   ],
   "source": [
    "# In-domain: product\n",
    "\n",
    "# Pre-trained: BERT, TaPas, TaBERT\n",
    "evaluate_masked_prediction(product, 'Key', bert_base, tokenizer)\n",
    "evaluate_masked_prediction(product, 'Key', tapas, tapas_tokenizer)\n",
    "\n",
    "evaluate_masked_prediction(product, 'Value', bert_base, tokenizer)\n",
    "evaluate_masked_prediction(product, 'Value', tapas, tapas_tokenizer)\n",
    "\n",
    "# Domain-specific pre-trained: Ours, No CL, No IP_a0, No IP_a1, No HEL, trained BERT\n",
    "evaluate_masked_prediction(product, 'Key', ours_product, tokenizer)\n",
    "evaluate_masked_prediction(product, 'Key', no_cl_product, tokenizer)\n",
    "evaluate_masked_prediction(product, 'Key', alpha_0_product, tokenizer)\n",
    "evaluate_masked_prediction(product, 'Key', alpha_1_product, tokenizer)\n",
    "evaluate_masked_prediction(product, 'Key', no_hel_product, tokenizer)\n",
    "evaluate_masked_prediction(product, 'Key', bert_product, tokenizer)\n",
    "\n",
    "evaluate_masked_prediction(product, 'Value', ours_product, tokenizer)\n",
    "evaluate_masked_prediction(product, 'Value', no_cl_product, tokenizer)\n",
    "evaluate_masked_prediction(product, 'Value', alpha_0_product, tokenizer)\n",
    "evaluate_masked_prediction(product, 'Value', alpha_1_product, tokenizer)\n",
    "evaluate_masked_prediction(product, 'Value', no_hel_product, tokenizer)\n",
    "evaluate_masked_prediction(product, 'Value', bert_product, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct / Total: 3160/3183\n",
      "Model Accuracy on Masked Key Prediction: 0.9928%\n",
      "Correct / Total: 2066/2849\n",
      "Model Accuracy on Masked Value Prediction: 0.7252%\n",
      "Correct / Total: 2647/3183\n",
      "Model Accuracy on Masked Key Prediction: 0.8316%\n",
      "Correct / Total: 1721/2849\n",
      "Model Accuracy on Masked Value Prediction: 0.6041%\n",
      "Correct / Total: 4082/4140\n",
      "Model Accuracy on Masked Key Prediction: 0.9860%\n",
      "Correct / Total: 1599/2490\n",
      "Model Accuracy on Masked Value Prediction: 0.6422%\n",
      "Correct / Total: 3130/4140\n",
      "Model Accuracy on Masked Key Prediction: 0.7560%\n",
      "Correct / Total: 1447/2490\n",
      "Model Accuracy on Masked Value Prediction: 0.5811%\n"
     ]
    }
   ],
   "source": [
    "# Cross-domain\n",
    "\n",
    "# Trained on Product -> Tested on Movie\n",
    "evaluate_masked_prediction(movie, 'Key', ours_product, tokenizer)\n",
    "evaluate_masked_prediction(movie, 'Value', ours_product, tokenizer)\n",
    "\n",
    "evaluate_masked_prediction(movie, 'Key', bert_product, tokenizer)\n",
    "evaluate_masked_prediction(movie, 'Value', bert_product, tokenizer)\n",
    "\n",
    "\n",
    "# Trained on Movie -> Tested on Product\n",
    "evaluate_masked_prediction(product, 'Key', ours_movie, tokenizer)\n",
    "evaluate_masked_prediction(product, 'Value', ours_movie, tokenizer)\n",
    "\n",
    "evaluate_masked_prediction(product, 'Key', bert_movie, tokenizer)\n",
    "evaluate_masked_prediction(product, 'Value', bert_movie, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "movie_for_cls_path = \"./data/movie_for_cls.csv\"\n",
    "adult_path = \"./data/adult.csv\"\n",
    "bank_path = \"./data/bank.csv\"\n",
    "heart_path = \"./data/heart.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate pre-trained (including domain-specific) models\n",
    "\n",
    "models = {\n",
    "    \"bert_base\": (bert_base, tokenizer),\n",
    "    \"tapas\": (tapas, tapas_tokenizer),\n",
    "    \"bert_product\": (bert_product, tokenizer),\n",
    "    \"bert_movie\": (bert_movie, tokenizer),\n",
    "    \"ours_product\": (ours_product, tokenizer),\n",
    "    \"ours_movie\": (ours_movie, tokenizer)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (611 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/tapas/tokenization_tapas.py:2699: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text = normalize_for_match(row[col_index].text)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/tapas/tokenization_tapas.py:1493: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  cell = row[col_index]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for bert_base in movie_for_cls:\n",
      "\t 0.6316\n",
      "\t 0.7500\n",
      "\t 0.6857\n",
      "Metrics for tapas in movie_for_cls:\n",
      "\t 0.7182\n",
      "\t 0.8229\n",
      "\t 0.7670\n",
      "Metrics for bert_product in movie_for_cls:\n",
      "\t 0.6364\n",
      "\t 0.7292\n",
      "\t 0.6796\n",
      "Metrics for bert_movie in movie_for_cls:\n",
      "\t 0.7419\n",
      "\t 0.6635\n",
      "\t 0.7005\n",
      "Metrics for ours_product in movie_for_cls:\n",
      "\t 0.6154\n",
      "\t 0.5385\n",
      "\t 0.5744\n",
      "Metrics for ours_movie in movie_for_cls:\n",
      "\t 0.6458\n",
      "\t 0.5962\n",
      "\t 0.6200\n"
     ]
    }
   ],
   "source": [
    "# Movie_test\n",
    "results = {}\n",
    "for name, (model, tokenizer) in models.items():\n",
    "    X_train, X_test, y_train, y_test = prepare_Xy(movie_for_cls_path, model, tokenizer, target='genre', seed=42)\n",
    "    results[name] = train_eval_rf(X_train, X_test, y_train, y_test, seed=42)\n",
    "\n",
    "# Print results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"Metrics for {model_name} in movie_for_cls:\")\n",
    "    print(f\"\\t{metrics['precision']: .4f}\")\n",
    "    print(f\"\\t{metrics['recall']: .4f}\")\n",
    "    print(f\"\\t{metrics['f1_score']: .4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/tapas/tokenization_tapas.py:2699: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text = normalize_for_match(row[col_index].text)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/tapas/tokenization_tapas.py:1493: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  cell = row[col_index]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for bert_base in adult:\n",
      "\t 0.8193\n",
      "\t 0.7083\n",
      "\t 0.7598\n",
      "Metrics for tapas in adult:\n",
      "\t 0.7615\n",
      "\t 0.7981\n",
      "\t 0.7793\n",
      "Metrics for bert_product in adult:\n",
      "\t 0.7614\n",
      "\t 0.6979\n",
      "\t 0.7283\n",
      "Metrics for bert_movie in adult:\n",
      "\t 0.7636\n",
      "\t 0.8077\n",
      "\t 0.7850\n",
      "Metrics for ours_product in adult:\n",
      "\t 0.6633\n",
      "\t 0.6250\n",
      "\t 0.6436\n",
      "Metrics for ours_movie in adult:\n",
      "\t 0.7455\n",
      "\t 0.7885\n",
      "\t 0.7664\n"
     ]
    }
   ],
   "source": [
    "# Adult\n",
    "results = {}\n",
    "for name, (model, tokenizer) in models.items():\n",
    "    X_train, X_test, y_train, y_test = prepare_Xy(adult_path, model, tokenizer, target='class', seed=42)\n",
    "    results[name] = train_eval_rf(X_train, X_test, y_train, y_test, seed=42)\n",
    "\n",
    "# Print results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"Metrics for {model_name} in adult:\")\n",
    "    print(f\"\\t{metrics['precision']: .4f}\")\n",
    "    print(f\"\\t{metrics['recall']: .4f}\")\n",
    "    print(f\"\\t{metrics['f1_score']: .4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/tapas/tokenization_tapas.py:2699: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text = normalize_for_match(row[col_index].text)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/tapas/tokenization_tapas.py:1493: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  cell = row[col_index]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for bert_base in bank:\n",
      "\t 0.7788\n",
      "\t 0.7788\n",
      "\t 0.7788\n",
      "Metrics for tapas in bank:\n",
      "\t 0.7449\n",
      "\t 0.7604\n",
      "\t 0.7526\n",
      "Metrics for bert_product in bank:\n",
      "\t 0.7810\n",
      "\t 0.7885\n",
      "\t 0.7847\n",
      "Metrics for bert_movie in bank:\n",
      "\t 0.8095\n",
      "\t 0.8173\n",
      "\t 0.8134\n",
      "Metrics for ours_product in bank:\n",
      "\t 0.7143\n",
      "\t 0.6250\n",
      "\t 0.6667\n",
      "Metrics for ours_movie in bank:\n",
      "\t 0.7113\n",
      "\t 0.6635\n",
      "\t 0.6866\n"
     ]
    }
   ],
   "source": [
    "# Bank\n",
    "results = {}\n",
    "for name, (model, tokenizer) in models.items():\n",
    "    X_train, X_test, y_train, y_test = prepare_Xy(bank_path, model, tokenizer, target='class', seed=42)\n",
    "    results[name] = train_eval_rf(X_train, X_test, y_train, y_test, seed=42)\n",
    "\n",
    "# Print results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"Metrics for {model_name} in bank:\")\n",
    "    print(f\"\\t{metrics['precision']: .4f}\")\n",
    "    print(f\"\\t{metrics['recall']: .4f}\")\n",
    "    print(f\"\\t{metrics['f1_score']: .4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/tapas/tokenization_tapas.py:2699: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text = normalize_for_match(row[col_index].text)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/tapas/tokenization_tapas.py:1493: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  cell = row[col_index]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 49.06 MiB is free. Process 3606157 has 3.18 GiB memory in use. Process 3682593 has 3.18 GiB memory in use. Process 3740249 has 17.15 GiB memory in use. Of the allocated memory 16.84 GiB is allocated by PyTorch, and 68.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, (model, tokenizer) \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 4\u001b[0m     X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_Xy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheart_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     results[name] \u001b[38;5;241m=\u001b[39m train_eval_rf(X_train, X_test, y_train, y_test, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/utils.py:290\u001b[0m, in \u001b[0;36mprepare_Xy\u001b[0;34m(path, model, tokenizer, target, seed)\u001b[0m\n\u001b[1;32m    287\u001b[0m train_data, test_data, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(data, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mseed)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# Extract embeddings\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m X_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([get_table_embedding(entry, model, tokenizer, target) \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m train_data])\n\u001b[1;32m    291\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([get_table_embedding(entry, model, tokenizer, target) \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m test_data])\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_train, X_test, y_train, y_test\n",
      "File \u001b[0;32m/workspace/utils.py:290\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    287\u001b[0m train_data, test_data, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(data, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mseed)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# Extract embeddings\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m X_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mget_table_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m train_data])\n\u001b[1;32m    291\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([get_table_embedding(entry, model, tokenizer, target) \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m test_data])\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_train, X_test, y_train, y_test\n",
      "File \u001b[0;32m/workspace/utils.py:253\u001b[0m, in \u001b[0;36mget_table_embedding\u001b[0;34m(entry, model, tokenizer, target)\u001b[0m\n\u001b[1;32m    251\u001b[0m     key_positions \u001b[38;5;241m=\u001b[39m _find_key_positions(serialized, entry, tokenizer)\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 253\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_positions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey_positions\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    254\u001b[0m     last_hidden_state \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/workspace/model_complete.py:142\u001b[0m, in \u001b[0;36mJSONBERT_COMPLETE.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, labels, key_positions, compute_alignment_loss, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key_positions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replace_key_embeddings(input_ids, key_positions, sequence_output)\n\u001b[0;32m--> 142\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    145\u001b[0m mlm_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:798\u001b[0m, in \u001b[0;36mBertOnlyMLMHead.forward\u001b[0;34m(self, sequence_output)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sequence_output: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 798\u001b[0m     prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prediction_scores\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:788\u001b[0m, in \u001b[0;36mBertLMPredictionHead.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m    787\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(hidden_states)\n\u001b[0;32m--> 788\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 23.57 GiB of which 49.06 MiB is free. Process 3606157 has 3.18 GiB memory in use. Process 3682593 has 3.18 GiB memory in use. Process 3740249 has 17.15 GiB memory in use. Of the allocated memory 16.84 GiB is allocated by PyTorch, and 68.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Heart\n",
    "results = {}\n",
    "for name, (model, tokenizer) in models.items():\n",
    "    X_train, X_test, y_train, y_test = prepare_Xy(heart_path, model, tokenizer, target='class', seed=42)\n",
    "    results[name] = train_eval_rf(X_train, X_test, y_train, y_test, seed=42)\n",
    "\n",
    "# Print results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"Metrics for {model_name} in heart:\")\n",
    "    print(f\"\\t{metrics['precision']: .4f}\")\n",
    "    print(f\"\\t{metrics['recall']: .4f}\")\n",
    "    print(f\"\\t{metrics['f1_score']: .4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
